[
    {
        "id": "vision_001",
        "title": "Text-Erkennung mit Vision",
        "description":
            "Nutze Apples Vision Framework, um Text in Bildern automatisch zu erkennen.",
        "steps": [
            "Vision importieren und eine VNRecognizeTextRequest erstellen.",
            "Bild in CIImage oder CGImage konvertieren.",
            "Erkannte Texte im Completion Handler ausgeben."
        ],
        "colors": {
            "backgroundColors": [
                "#000000",
                "#5E5CE6",
                "#000000"
            ],
            "textColors": [
                "#FFFFFF"
            ]
        },
        "code":
            "import Vision\nimport UIKit\n\nlet image = UIImage(named: \"text_image\")!\nlet cgImage = image.cgImage!\n\nlet request = VNRecognizeTextRequest { request, _ in\n    guard let results = request.results as? [VNRecognizedTextObservation] else { return }\n    for observation in results {\n        if let top = observation.topCandidates(1).first {\n            print(\"ðŸ“„ Erkannter Text: \\(top.string)\")\n        }\n    }\n}\n\nlet handler = VNImageRequestHandler(cgImage: cgImage)\ntry? handler.perform([request])",
        "category": "Vision",
        "categoryIcon": "vision.pro",
        "categoryIconColor": "#5E5CE6"
    },
    {
      "id": "vision_002",
      "title": "Gesichtserkennung (Face Detection)",
      "description": "Erkenne Gesichter in Bildern mithilfe von Vision und markiere ihre Bereiche.",
      "steps": [
        "Erstelle eine VNDetectFaceRectanglesRequest.",
        "Konvertiere dein UIImage in CGImage.",
        "Lies boundingBox der erkannten Gesichter aus."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#4B7BFF", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\nimport UIKit\n\nlet image = UIImage(named: \"person\")!\nlet cgImage = image.cgImage!\n\nlet request = VNDetectFaceRectanglesRequest { req, _ in\n    guard let faces = req.results as? [VNFaceObservation] else { return }\n    for face in faces {\n        print(\"ðŸ˜€ Gesicht erkannt: \\(face.boundingBox)\")\n    }\n}\n\nlet handler = VNImageRequestHandler(cgImage: cgImage)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#4B7BFF"
    },
    {
      "id": "vision_003",
      "title": "Objekt-Erkennung (VNRecognizedObjectRequest)",
      "description": "Erkenne vordefinierte Objekte wie Menschen, Tiere, Fahrzeuge usw.",
      "steps": [
        "Erstelle eine VNRecognizedObjectRequest.",
        "Nutze ein Core ML Modell (z. B. YOLO).",
        "Gib erkannte Objekte mit Confidence aus."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#00A8E8", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\nimport CoreML\n\nlet model = try! VNCoreMLModel(for: YOLOv3().model)\nlet request = VNRecognizedObjectRequest(model: model) { req, _ in\n    guard let results = req.results as? [VNRecognizedObjectObservation] else { return }\n    for obj in results {\n        let label = obj.labels.first?.identifier ?? \"?\"\n        let confidence = obj.labels.first?.confidence ?? 0\n        print(\"ðŸ· Objekt: \\(label) â€“ \\(confidence)\")\n    }\n}",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#00A8E8"
    },
    {
      "id": "vision_004",
      "title": "Barcode-Erkennung (QR, EAN, PDF417)",
      "description": "Scanne Barcodes direkt aus einem Bild â€“ QR, EAN, UPC und viele weitere.",
      "steps": [
        "VNDetectBarcodesRequest erstellen.",
        "Bild Ã¼ber VNImageRequestHandler analysieren.",
        "Barcode-Inhalt ausgeben."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#28CD41", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\nimport UIKit\n\nlet request = VNDetectBarcodesRequest { req, _ in\n    guard let barcodes = req.results as? [VNBarcodeObservation] else { return }\n    for code in barcodes {\n        print(\"ðŸ” Barcode: \\(code.payloadStringValue ?? \"?\")\")\n    }\n}\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"qr\")!.cgImage!)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#28CD41"
    },
    {
      "id": "vision_005",
      "title": "Handpose-Erkennung (VNDetectHumanHandPoseRequest)",
      "description": "Analysiere Handpositionen und Fingerpunkte â€“ perfekt fÃ¼r AR- oder Gestensteuerung.",
      "steps": [
        "Erstelle eine VNDetectHumanHandPoseRequest.",
        "Analysiere Landmark-Punkte wie Daumen oder Zeigefinger.",
        "Nutze diese Daten fÃ¼r Gestenerkennung."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#FF9F0A", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\n\nlet request = VNDetectHumanHandPoseRequest { req, _ in\n    guard let hands = req.results as? [VNHumanHandPoseObservation] else { return }\n    for hand in hands {\n        if let thumb = try? hand.recognizedPoint(.thumbTip) {\n            print(\"ðŸ‘ Daumen erkannt: \\(thumb.location)\")\n        }\n    }\n}\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"hand\")!.cgImage!)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#FF9F0A"
    },
    {
      "id": "vision_006",
      "title": "KÃ¶rpererkennung (Human Body Pose)",
      "description": "Erkenne KÃ¶rperhaltung und Skelettpunkte â€“ ideal fÃ¼r Fitness-Apps oder Animation.",
      "steps": [
        "VNDetectHumanBodyPoseRequest nutzen.",
        "Landmarks wie HÃ¼fte, Schultern oder Knie auslesen.",
        "Pose-Daten weiterverarbeiten."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#FF453A", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\n\nlet request = VNDetectHumanBodyPoseRequest { req, _ in\n    guard let poses = req.results as? [VNHumanBodyPoseObservation] else { return }\n    for pose in poses {\n        if let leftHand = try? pose.recognizedPoint(.leftWrist) {\n            print(\"âœ‹ Linkes Handgelenk: \\(leftHand.location)\")\n        }\n    }\n}\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"body\")!.cgImage!)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#FF453A"
    },
    {
      "id": "vision_007",
      "title": "Saliency Detection (Bild-Fokuspunkte)",
      "description": "Erkenne automatisch wichtige Bereiche eines Bildes â€“ ideal fÃ¼r Cropping oder Fokus.",
      "steps": [
        "VNGenerateAttentionBasedSaliencyImageRequest erstellen.",
        "Heatmap der wichtigen Bereiche ausgeben.",
        "Bounding Box extrahieren."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#AD51FF", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\n\nlet request = VNGenerateAttentionBasedSaliencyImageRequest { req, _ in\n    if let result = req.results?.first as? VNSaliencyImageObservation {\n        print(\"ðŸ”¥ Wichtigste Region: \\(result.boundingBox)\")\n    }\n}\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"scene\")!.cgImage!)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#AD51FF"
    },
    {
      "id": "vision_008",
      "title": "Texterkennung fÃ¼r mehrere Sprachen",
      "description": "Erkenne Text in Bildern und lege mehrere Sprachen fest.",
      "steps": [
        "VNRecognizeTextRequest mit recognitionLanguages konfigurieren.",
        "Mehrere Sprachen gleichzeitig analysieren.",
        "Erkannte Texte ausgeben."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#32ADE6", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\n\nlet request = VNRecognizeTextRequest { req, _ in\n    for result in (req.results as? [VNRecognizedTextObservation]) ?? [] {\n        if let text = result.topCandidates(1).first?.string {\n            print(\"ðŸŒ Text erkannt: \\(text)\")\n        }\n    }\n}\n\nrequest.recognitionLanguages = [\"de-DE\", \"en-US\", \"tr-TR\"]\nrequest.usesLanguageCorrection = true\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"text_multi\")!.cgImage!)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#32ADE6"
    },
    {
      "id": "vision_009",
      "title": "Contours â€“ Linien & Umriss-Erkennung",
      "description": "Erkenne Kanten, Shapes und Umrisse in einem Bild.",
      "steps": [
        "VNDetectContoursRequest erstellen.",
        "Minimale LinienlÃ¤nge und Empfindlichkeit einstellen.",
        "Als Paths ausgeben."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#FFD60A", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\n\nlet request = VNDetectContoursRequest { req, _ in\n    guard let obs = req.results?.first as? VNContoursObservation else { return }\n    print(\"ðŸŒ€ Gesamt-Linien: \\(obs.contourCount)\")\n}\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"shape\")!.cgImage!)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#FFD60A"
    },
    {
      "id": "vision_010",
      "title": "Face Landmark Detection (Augen, Nase, Mund)",
      "description": "Erkenne prÃ¤zise Gesichtsmerkmale wie Augen, Nase, Mund und Kiefer.",
      "steps": [
        "Nutze VNDetectFaceLandmarksRequest.",
        "Lese Punkte wie leftEye oder nose aus.",
        "Ideal fÃ¼r Filter-Apps."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#F03E53", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\nimport UIKit\n\nlet request = VNDetectFaceLandmarksRequest { req, _ in\n    guard let faces = req.results as? [VNFaceObservation] else { return }\n    for face in faces {\n        if let landmarks = face.landmarks?.leftEye {\n            print(\"ðŸ‘ Linkes Auge Punkte: \\(landmarks.normalizedPoints.count)\")\n        }\n    }\n}\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"face\")!.cgImage!)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#F03E53"
    },
    {
      "id": "vision_011",
      "title": "Objekt-Tracking in Videos (VNTrackObjectRequest)",
      "description": "Verfolge ein Objekt Ã¼ber mehrere Videoframes hinweg â€“ perfekt fÃ¼r AR und Analyse.",
      "steps": [
        "Erstelle VNDetectedObjectObservation.",
        "Initialisiere Tracking-Request.",
        "Aktualisiere die Bounding Box pro Frame."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#7C3AED", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\nimport AVFoundation\n\nlet initial = VNDetectedObjectObservation(boundingBox: CGRect(x: 0.4, y: 0.4, width: 0.2, height: 0.2))\n\nlet request = VNTrackObjectRequest(detectedObjectObservation: initial)\nrequest.trackingLevel = .accurate\n\nfunc handleFrame(_ buffer: CVPixelBuffer) {\n    let handler = VNImageRequestHandler(cvPixelBuffer: buffer)\n    try? handler.perform([request])\n\n    if let result = request.results?.first as? VNDetectedObjectObservation {\n        print(\"ðŸŽ¯ Position: \\(result.boundingBox)\")\n    }\n}",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#7C3AED"
    },
    {
      "id": "vision_012",
      "title": "Dokumenten-Segmentierung (Scanner)",
      "description": "Erkenne DokumentrÃ¤nder automatisch â€“ ideal fÃ¼r Scanner-Apps wie Adobe Scan.",
      "steps": [
        "VNDetectDocumentSegmentationRequest erstellen.",
        "Umrisspunkte extrahieren.",
        "Dokument entzerren oder zuschneiden."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#0A84FF", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\nimport UIKit\n\nlet request = VNDetectDocumentSegmentationRequest { req, _ in\n    guard let result = req.results?.first as? VNDocumentSegmentationObservation else { return }\n    print(\"ðŸ“„ Dokument erkannt mit \\(result.points.count) Punkten\")\n}\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"doc\")!.cgImage!)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#0A84FF"
    },
    {
      "id": "vision_013",
      "title": "Personensegmentierung (Hintergrund entfernen)",
      "description": "Extrahiere Personen aus Bildern â€“ perfekt fÃ¼r Portrait-Effekte, AI-Editing oder AR.",
      "steps": [
        "VNGeneratePersonSegmentationRequest nutzen.",
        "Confidence Map auslesen.",
        "Maske auf Bild anwenden (Matte)."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#34C759", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\nimport UIKit\n\nlet request = VNGeneratePersonSegmentationRequest()\nrequest.qualityLevel = .accurate\nrequest.outputPixelFormat = kCVPixelFormatType_OneComponent8\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"person\")!.cgImage!)\ntry? handler.perform([request])\n\nif let mask = request.results?.first as? VNPixelBufferObservation {\n    print(\"ðŸ§ Maske erzeugt: \\(mask.pixelBuffer)\")\n}",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#34C759"
    },
    {
      "id": "vision_014",
      "title": "CoreML Segmentierung (Objektmasken)",
      "description": "Nutze ein CoreML-Segmentation-Modell, um Pixel genau zu klassifizieren (Person, Himmel, Haare usw.).",
      "steps": [
        "CoreML-Segmentation-Model laden.",
        "VNCoreMLRequest erstellen.",
        "Pixel-Labels ausgeben."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#FF3B30", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\nimport CoreML\n\nlet model = try! VNCoreMLModel(for: DeepLabV3().model)\nlet request = VNCoreMLRequest(model: model) { req, _ in\n    if let result = req.results?.first as? VNPixelBufferObservation {\n        print(\"ðŸ–¼ Segmentation PixelBuffer: \\(result.pixelBuffer)\")\n    }\n}\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"scene\")!.cgImage!)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#FF3B30"
    },
    {
      "id": "vision_015",
      "title": "Text-Layout Erkennung (TextblÃ¶cke finden)",
      "description": "Finde automatisch TextblÃ¶cke, Ãœberschriften und Layout-Strukturen in einem Dokument.",
      "steps": [
        "VNDetectTextRectanglesRequest erstellen.",
        "Bounding Boxes pro Textblock auslesen.",
        "Perfekt fÃ¼r OCR-Vorverarbeitung."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#FF9F0A", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\nimport UIKit\n\nlet request = VNDetectTextRectanglesRequest { req, _ in\n    guard let boxes = req.results as? [VNTextObservation] else { return }\n    for box in boxes {\n        print(\"ðŸŸ¦ Textblock gefunden: \\(box.boundingBox)\")\n    }\n}\nrequest.reportCharacterBoxes = true\n\nlet handler = VNImageRequestHandler(cgImage: UIImage(named: \"document\")!.cgImage!)\ntry? handler.perform([request])",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#FF9F0A"
    },
    {
      "id": "vision_016",
      "title": "Bild-Ã„hnlichkeit (Image Similarity)",
      "description": "Vergleiche zwei Bilder und berechne ihre visuelle Ã„hnlichkeit. Ideal fÃ¼r KI-Apps oder Fotoverwaltung.",
      "steps": [
        "VNGenerateImageFeaturePrintRequest fÃ¼r beide Bilder ausfÃ¼hren.",
        "Feature Vektoren extrahieren.",
        "distance() berechnen â†’ je kleiner, desto Ã¤hnlicher."
      ],
      "colors": {
        "backgroundColors": ["#000000", "#7C3AED", "#000000"],
        "textColors": ["#FFFFFF"]
      },
      "code": "import Vision\nimport UIKit\n\nfunc featurePrint(from image: UIImage) -> VNFeaturePrintObservation? {\n    let req = VNGenerateImageFeaturePrintRequest()\n    let handler = VNImageRequestHandler(cgImage: image.cgImage!)\n    try? handler.perform([req])\n    return req.results?.first as? VNFeaturePrintObservation\n}\n\nlet img1 = UIImage(named: \"photo1\")!\nlet img2 = UIImage(named: \"photo2\")!\n\nif let f1 = featurePrint(from: img1), let f2 = featurePrint(from: img2) {\n    var distance: Float = 0\n    try? f1.computeDistance(&distance, to: f2)\n    print(\"ðŸ”Ž Ã„hnlichkeit: \\(distance)\")\n}",
      "category": "Vision",
      "categoryIcon": "vision.pro",
      "categoryIconColor": "#7C3AED"
    }
]
