[
    {
        "id": "speech_001",
        "title": "Spracherkennung (Speech Framework)",
        "description":
            "Nutze Apples Speech API, um gesprochene Sprache in Text umzuwandeln.",
        "steps": [
            "Speech importieren und SFSpeechRecognizer erstellen.",
            "Mikrofonzugriff anfordern.",
            "AudioSession starten und Texterkennung aktivieren."
        ],
        "colors": {
            "backgroundColors": [
                "#000000",
                "#30D158",
                "#000000"
            ],
            "textColors": [
                "#FFFFFF"
            ]
        },
        "code":
            "import Speech\n\nlet recognizer = SFSpeechRecognizer(locale: Locale(identifier: \"de-DE\"))!\nSFSpeechRecognizer.requestAuthorization { status in\n    if status == .authorized {\n        let request = SFSpeechURLRecognitionRequest(url: URL(filePath: \"audio.m4a\"))\n        recognizer.recognitionTask(with: request) { result, _ in\n            if let text = result?.bestTranscription.formattedString {\n                print(\"üó£ Erkannt: \\(text)\")\n            }\n        }\n    } else {\n        print(\"üö´ Kein Zugriff auf Spracherkennung\")\n    }\n}",
        "category": "Speech",
        "categoryIcon": "microphone",
        "categoryIconColor": "#30D158"
    },
    {
        "id": "speech_002",
        "title": "Live-Spracherkennung (Mikrofon-Input)",
        "description": "Erkenne Sprache live √ºber das Mikrofon und konvertiere sie in Text.",
        "steps": [
            "SFSpeechAudioBufferRecognitionRequest nutzen.",
            "AudioSession konfigurieren (Record, Measurement).",
            "AudioEngine starten und Input in den RecognitionRequest streamen.",
            "Transkription laufend aktualisieren."
        ],
        "colors": {
            "backgroundColors": ["#000000", "#0A84FF", "#000000"],
            "textColors": ["#FFFFFF"]
        },
        "code":
            "import AVFoundation\nimport Speech\n\nclass SpeechLiveRecognizer {\n    private let audioEngine = AVAudioEngine()\n    private let recognizer = SFSpeechRecognizer(locale: Locale(identifier: \"de-DE\"))!\n    private var request: SFSpeechAudioBufferRecognitionRequest?\n    private var task: SFSpeechRecognitionTask?\n\n    func start() throws {\n        SFSpeechRecognizer.requestAuthorization { status in\n            guard status == .authorized else { return }\n        }\n\n        let session = AVAudioSession.sharedInstance()\n        try session.setCategory(.record, mode: .measurement, options: .duckOthers)\n        try session.setActive(true)\n\n        request = SFSpeechAudioBufferRecognitionRequest()\n\n        let input = audioEngine.inputNode\n        input.installTap(onBus: 0, bufferSize: 1024, format: input.outputFormat(forBus: 0)) { buffer, _ in\n            self.request?.append(buffer)\n        }\n\n        audioEngine.prepare()\n        try audioEngine.start()\n\n        task = recognizer.recognitionTask(with: request!) { result, error in\n            if let text = result?.bestTranscription.formattedString {\n                print(\"üéô Live: \\(text)\")\n            }\n        }\n    }\n}",
        "category": "Speech",
        "categoryIcon": "microphone",
        "categoryIconColor": "#0A84FF"
    },
    {
        "id": "speech_003",
        "title": "Offline-Spracherkennung pr√ºfen",
        "description": "Pr√ºfe, ob dein Ger√§t Offline-Spracherkennung unterst√ºtzt.",
        "steps": [
            "SFSpeechRecognizer.supportsOnDeviceRecognition nutzen.",
            "Ideal f√ºr Privatsph√§re & schnelle Verarbeitung.",
            "Nur in bestimmten Sprachen verf√ºgbar."
        ],
        "colors": {
            "backgroundColors": ["#000000", "#FF9500", "#000000"],
            "textColors": ["#FFFFFF"]
        },
        "code":
            "import Speech\n\nlet recognizer = SFSpeechRecognizer(locale: Locale(identifier: \"de-DE\"))!\n\nif recognizer.supportsOnDeviceRecognition {\n    print(\"üì¥ Offline-Erkennung verf√ºgbar!\")\n} else {\n    print(\"‚òÅÔ∏è Nur Online-Erkennung m√∂glich\")\n}",
        "category": "Speech",
        "categoryIcon": "microphone",
        "categoryIconColor": "#FF9500"
    },
    {
        "id": "speech_004",
        "title": "Sprach-Segment-Analyse (W√∂rter & Zeitstempel)",
        "description": "Analysiere jedes erkannte Wort mit Zeitstempeln ‚Äì perfekt f√ºr Untertitel / Karaoke.",
        "steps": [
            "Auf result.bestTranscription.segments zugreifen.",
            "Segment enth√§lt: Wort, Offset, Dauer.",
            "Kann live aktualisiert werden."
        ],
        "colors": {
            "backgroundColors": ["#000000", "#30D158", "#000000"],
            "textColors": ["#FFFFFF"]
        },
        "code":
            "import Speech\n\nlet recognizer = SFSpeechRecognizer(locale: Locale(identifier: \"de-DE\"))!\nlet request = SFSpeechURLRecognitionRequest(url: URL(filePath: \"audio.m4a\"))\n\nrecognizer.recognitionTask(with: request) { result, _ in\n    guard let segments = result?.bestTranscription.segments else { return }\n\n    for segment in segments {\n        print(\"üó£ Wort: \\(segment.substring)\")\n        print(\"‚è± Startzeit: \\(segment.timestamp)\")\n        print(\"‚è≥ Dauer: \\(segment.duration)\")\n        print(\"---\")\n    }\n}",
        "category": "Speech",
        "categoryIcon": "microphone",
        "categoryIconColor": "#30D158"
    },
    {
        "id": "speech_005",
        "title": "Spracherkennung abbrechen / pausieren",
        "description": "Stoppe oder pausiere laufende Erkennung ‚Äì wichtig f√ºr UI-Control (Record, Stop).",
        "steps": [
            "AudioEngine stoppen.",
            "RecognitionTask canceln.",
            "Request beenden oder zur√ºcksetzen."
        ],
        "colors": {
            "backgroundColors": ["#000000", "#FF2D55", "#000000"],
            "textColors": ["#FFFFFF"]
        },
        "code":
            "func stopRecognition() {\n    audioEngine.stop()\n    audioEngine.inputNode.removeTap(onBus: 0)\n    request?.endAudio()\n    task?.cancel()\n    print(\"üõë Erkennung gestoppt\")\n}",
        "category": "Speech",
        "categoryIcon": "microphone",
        "categoryIconColor": "#FF2D55"
    },
    {
        "id": "speech_006",
        "title": "Spracherkennung in SwiftUI integrieren",
        "description": "Ein komplettes SwiftUI Beispiel: Button ‚Üí Start/Stop ‚Üí Live Text.",
        "steps": [
            "ObservableObject f√ºr Speech-Manager.",
            "Live-Text als Published Property.",
            "SwiftUI Button steuert Aufnahme."
        ],
        "colors": {
            "backgroundColors": ["#000000", "#61DAFB", "#000000"],
            "textColors": ["#FFFFFF"]
        },
        "code":
            "class SpeechManager: NSObject, ObservableObject {\n    @Published var text = \"Sag etwas‚Ä¶\"\n    // Starte hier AudioEngine & Task (aus speech_002)\n}\n\nstruct SpeechView: View {\n    @StateObject private var speech = SpeechManager()\n\n    var body: some View {\n        VStack(spacing: 40) {\n            Text(speech.text)\n                .font(.title2)\n                .foregroundColor(.white)\n                .padding()\n\n            Button(\"üé§ Start\") {\n                try? speech.start()\n            }\n            .buttonStyle(.borderedProminent)\n        }\n    }\n}",
        "category": "Speech",
        "categoryIcon": "microphone",
        "categoryIconColor": "#61DAFB"
    },
    {
        "id": "speech_007",
        "title": "Speech ‚Üí Commands (Befehle erkennen)",
        "description": "Erkenne Befehle wie ‚ÄûAttack‚Äú, ‚ÄûHeal‚Äú, ‚ÄûNext Level‚Äú ‚Äì perfekt f√ºr dein Slayken-Gameplay!",
        "steps": [
            "Nach jedem Transkript auf Schl√ºsselw√∂rter pr√ºfen.",
            "Mehrere Synonyme unterst√ºtzen.",
            "Aktionen in der App triggern."
        ],
        "colors": {
            "backgroundColors": ["#000000", "#0A84FF", "#000000"],
            "textColors": ["#FFFFFF"]
        },
        "code":
            "func handleSpeechCommand(_ text: String) {\n    let lower = text.lowercased()\n\n    if lower.contains(\"attack\") || lower.contains(\"angriff\") {\n        print(\"‚öîÔ∏è Attack ausgel√∂st!\")\n    }\n    if lower.contains(\"heal\") || lower.contains(\"heilen\") {\n        print(\"üíö Heal ausgel√∂st!\")\n    }\n    if lower.contains(\"next\") || lower.contains(\"weiter\") {\n        print(\"‚û°Ô∏è Next Level!\")\n    }\n}",
        "category": "Speech",
        "categoryIcon": "microphone",
        "categoryIconColor": "#0A84FF"
    }
]
